{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from npeet import entropy_estimators as ee  # alternative import for entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated average mutual information (in nats): -51.15008732364364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_average_mutual_information_np(filename, sigma, k_neighbors=3):\n",
    "    \"\"\"\n",
    "    Compute the average mutual information I_c from trajectory data.\n",
    "    \n",
    "    The file is assumed to be a UTF-16 encoded text file with shape (n, 2*M),\n",
    "    In each pair:\n",
    "      - The even-indexed columns (0,2,4,...) are the X trajectories.\n",
    "      - The odd-indexed columns (1,3,5,...) are the corresponding Y trajectories.\n",
    "      \n",
    "    The formula is:\n",
    "    \n",
    "      I_c = - n * ln(√(2π)*σ)\n",
    "            - (1/(2σ² * M)) * Σₘ Σₖ (xₘₖ - yₘₖ)²\n",
    "            + Σₖ H(P[yₖ])\n",
    "            \n",
    "    where H(P[yₖ]) is the Shannon entropy (in nats) of the distribution\n",
    "    of the Y values at time step k, estimated nonparametrically via a k-nearest-neighbor (KNN) estimator.\n",
    "    \n",
    "    Parameters:\n",
    "      filename    : str\n",
    "          Path to the data file (trajectories.txt).\n",
    "      sigma       : float\n",
    "          The noise standard deviation.\n",
    "      k_neighbors : int, optional\n",
    "          Number of nearest neighbors for the entropy estimator (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "      I_c_est     : float\n",
    "          The estimated average mutual information (in nats).\n",
    "\n",
    "    The specific function that we are using is : \n",
    "\n",
    "    \n",
    "def entropy(x, k=3, base=2):\n",
    "    The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "    x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = np.asarray(x)\n",
    "    n_elements, n_features = x.shape\n",
    "    x = add_noise(x)\n",
    "    tree = build_tree(x)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    const = digamma(n_elements) - digamma(k) + n_features * log(2)\n",
    "    return (const + n_features * np.log(nn).mean()) / log(base)\n",
    "    \"\"\"\n",
    "    # Load data with UTF-16 encoding\n",
    "    data = np.loadtxt(filename, encoding=\"utf-16\")\n",
    "    \n",
    "    # Get dimensions: n time steps and total_cols = 2*M columns\n",
    "    n, total_cols = data.shape\n",
    "    M = total_cols // 2  # number of trajectory pairs\n",
    "\n",
    "    # --- Term 1: Constant from the Gaussian factor ---\n",
    "    term1 = - n * math.log(math.sqrt(2 * math.pi) * sigma)\n",
    "    \n",
    "    # --- Term 2: Sum over squared differences (X - Y) for all trajectories and time steps ---\n",
    "    sum_sq_diff = 0.0\n",
    "    for m in range(M):\n",
    "        x_traj = data[:, 2*m]       # X trajectory for run m\n",
    "        y_traj = data[:, 2*m + 1]     # Corresponding Y trajectory\n",
    "        sum_sq_diff += np.sum((x_traj - y_traj)**2)\n",
    "    term2 = - (1.0 / (2 * sigma**2 * M)) * sum_sq_diff\n",
    "\n",
    "    # --- Term 3: Sum over time steps of the entropy of the y-values ---\n",
    "    # For each time step, collect all Y values from each trajectory and estimate the entropy.\n",
    "    entropy_sum = 0.0\n",
    "    for k in range(n):\n",
    "        # Extract all y-values at time step k from all trajectories (columns 1, 3, 5, ...)\n",
    "        y_k = data[k, 1::2].reshape(-1, 1)  # reshape to 2D: (M,1)\n",
    "        # Estimate entropy using the k-nearest neighbor estimator (returns entropy in nats)\n",
    "        H_k = ee.entropy(y_k, k=k_neighbors)\n",
    "        entropy_sum += H_k\n",
    "    term3 = entropy_sum\n",
    "\n",
    "    # --- Combine terms ---\n",
    "    #I_c_est = term1 + term2 + term3\n",
    "    I_c_est = term2\n",
    "    return I_c_est\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    filename = \"trajectories.txt\"\n",
    "    sigma = 1/64.565\n",
    "    time = 50\n",
    "    I_c_estimated = compute_average_mutual_information_np(filename, sigma, k_neighbors=3)/time\n",
    "    print(\"Estimated average mutual information (in nats):\", I_c_estimated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
